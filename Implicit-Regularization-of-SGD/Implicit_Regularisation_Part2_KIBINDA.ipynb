{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Implicit Regularisation Part2_KIBINDA",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENvaSjI1rWMO"
      },
      "source": [
        "**1. How deep learning works at all**\n",
        "\n",
        "To figure out how deep learning works, it's not sufficient to focus on the loss function or the model class only. During training step, many algorithms are used to find minima namely: Gradient Descente (GD), Mini-Batch Gradient Descente (MBGD) or Stochastic Gradient Descente (SGD). But SGD seems to play an important role.\n",
        "\n",
        "For the specific training data, there are several minima, some of them generalise well (ie result in low test error) others can be arbitrarly badly overfit.\n",
        "\n",
        "In this kind of situation, one is interested in whether the optimisation algorithm converges quickly to a local minimum (such as a general principle) but here the interest is in which of the available minima it prefers to reach first. \n",
        "\n",
        "Optimisation algorithms have certain preference in their convergence to a minimum among the possible available minima and this preference is often described as an \"implicit regularization\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-A2S0tNnxml7"
      },
      "source": [
        "**2. State of the art**\n",
        "\n",
        "When we train the deep learning model, the learning rate plays in some case an important role. Managing the learning rate can significantly achieve the performance both in test and train accuracies. Large learning rate can give the high test accuracy and this effect can minimize the training loss. It's often difficult to generalize this phenomenom, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss.\n",
        "\n",
        "To interpret this phenomenom, we use the SGD by modifying the loss function. \n",
        "\n",
        "This modification consists to combine the original loss function with an implicit regularizer which penalizes the norms of the Mini-Batch Gradients. All this modification is done under some assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size.\n",
        "\n",
        "The modified loss is:\n",
        "\n",
        "$$\\tilde{C}_{SGD}(w) = C(w) + \\dfrac{\\epsilon}{4m}\\displaystyle\\sum_{j = 1}^{m}\\|{\\nabla \\hat{C}_j(w)}\\|^2$$\n",
        "\n",
        "Where $m = \\dfrac{N}{B}$ defines the number of batches per epoch, B: batch size and N: The training set size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxfQzhWxAM3P"
      },
      "source": [
        "**3. Core idea**\n",
        "\n",
        "Modified the loss function in order to see how the small and finite learning rate can aid generalization.\n",
        "\n",
        "This novel approach is called **implicit regularization**, this technique is established for analysing optimization algorithms (especially SGD) with finite and small step or learning rate. SGD with random shuffling, iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkKjorrwEDDO"
      },
      "source": [
        "**4. Some mathematical elements**\n",
        "\n",
        "\n",
        "\n",
        "$w_{t + n} = w_{t} + \\alpha \\tilde{f}(w_{t}) + \\alpha \\tilde{f}(w_{t + 1}) + \\alpha \\tilde{f}(w_{t + 2})+ ...$\n",
        "\n",
        "$w_{t + n} = w_{t} + \\alpha \\tilde{f}(w_{t}) + \\alpha \\tilde{f}(w_{t} + \\alpha \\tilde{f}(w_{t})) + \\alpha \\tilde{f}(w_{t} + \\alpha \\tilde{f}(w_{t}) + \\alpha \\tilde{f}(w_{t} + \\alpha \\tilde{f}(w_{t}))) + ...$\n",
        "\n",
        "\n",
        "$$w_{t + n} = w_{t} + n \\alpha \\tilde{f}(w_{t}) + \\dfrac{n(n-1)}{2} \\alpha^2 \\nabla \\tilde{f}(w_{t})\\tilde{f}(w_{t}) + O(n^3 \\alpha^3) \\quad \\quad \\quad            (1)$$\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "$$w_{t + \\epsilon} = w_{t} + \\epsilon \\tilde{f}(w_{t}) + (\\epsilon^2/2)\\nabla \\tilde{f}(w_{t})\\tilde{f}(w_{t}) + O(\\epsilon^3)$$\n",
        "\n",
        "$$w_{t + \\epsilon} = w_{t} + \\epsilon f(w_{t}) + \\epsilon^2(f_{1}(w_{t}) + (1/2)\\nabla f(w_{t})f(w_{t})) + O(\\epsilon^3) \\quad \\quad (2)$$\n",
        "\n",
        "\n",
        "\n",
        "$$w_m = w_0 - \\epsilon \\nabla \\hat{C}_0(w_0) - \\epsilon \\nabla \\hat{C}_1(w_1) - \\epsilon \\nabla \\hat{C}_2(w_2) - ...$$\n",
        "\n",
        "$$w_m = w_0 - \\epsilon \\displaystyle\\sum_{j = 0}^{m - 1} \\nabla \\hat{C}_j(w_0) + \\epsilon^2 \\displaystyle\\sum_{j = 0}^{m - 1} \\displaystyle\\sum_{k < j} \\nabla \\nabla \\hat{C}_j(w_0) \\nabla \\hat{C}_k(w_0) + O(m^3 \\epsilon^3) $$\n",
        "\n",
        "$$w_m = w_0 - m\\epsilon \\nabla C(w_0) + \\epsilon^2 \\xi (w_0) + O(m^3 \\epsilon^3)$$\n",
        "\n",
        "\n",
        "$$w_{i+1} = w_i - \\epsilon \\nabla \\hat{C}_{i\\% m}(w_i)$$\n",
        "\n",
        "\n",
        "$$\\xi (w) = \\displaystyle\\sum_{j = 0}^{m - 1} \\displaystyle\\sum_{k < j} \\nabla \\nabla \\hat{C}_j(w) \\nabla \\hat{C}_k(w)$$\n",
        "\n",
        "\n",
        "\n",
        "$$\\mathbb{E} (\\xi (w)) = \\dfrac{1}{2} \\left( \\displaystyle\\sum_{j = 0}^{m - 1} \\displaystyle\\sum_{k \\neq j} \\nabla \\nabla \\hat{C}_j(w) \\nabla \\hat{C}_k(w) \\right)$$\n",
        "\n",
        "$$\\qquad \\qquad \\qquad \\qquad \\qquad \\qquad = \\dfrac{1}{2} \\nabla \\displaystyle\\sum_{j = 0}^{m - 1} \\nabla \\hat{C}_j(w) \\displaystyle\\sum_{k = 0}^{m - 1} \\nabla \\hat{C}_k(w) - \\dfrac{1}{2} \\nabla \\displaystyle\\sum_{j = 0}^{m - 1} \\nabla \\hat{C}_j(w) \\nabla \\hat{C}_j(w)$$\n",
        "\n",
        "$$\\quad \\quad \\quad \\quad \\qquad \\quad \\quad = \\dfrac{m^2}{4} \\nabla \\left( \\| \\nabla C(w) \\|^2 - \\dfrac{1}{m^2} \\displaystyle\\sum_{j = 0}^{m - 1} \\| \\nabla \\hat{C}_j(w) \\|^2 \\right)$$\n",
        "\n",
        "\n",
        "$$\\mathbb{E} (w_m) = w_0 - m \\epsilon \\nabla C(w_0) + \\dfrac{m^2 \\epsilon^2}{4} \\nabla \\left( \\| \\nabla C(w_0) \\|^2 - \\dfrac{1}{m^2} \\displaystyle\\sum_{j = 0}^{m - 1} \\| \\nabla \\hat{C}_j(w_0) \\|^2 \\right) + O(m^3 \\epsilon^3) \\qquad \\quad (3)$$\n",
        "\n",
        "\n",
        "$$w(m \\epsilon) = w_0 - m \\epsilon \\nabla C(w_0) + m^2\\epsilon^2 \\left( f_1(w_0) + \\dfrac{1}{2} \\nabla \\nabla C(w_0) \\nabla C(w_0) \\right) + O(m^3\\epsilon^3)$$\n",
        "\n",
        "$$\\qquad \\qquad \\qquad \\quad = w_0 - m \\epsilon \\nabla C(w_0) + m^2\\epsilon^2 \\left( f_1(w_0) + \\dfrac{1}{4} \\nabla \\| \\nabla C(w_0) \\|^2 \\right) + O(m^3\\epsilon^3) \\qquad \\qquad (4)$$\n",
        "\n",
        "\n",
        "\n",
        "$$\\mathbb{E} (w_m) = w(m\\epsilon) + O(m^3\\epsilon^3)$$\n",
        "\n",
        "\n",
        "$$ \\dot{w} = - \\nabla C(w) + m\\epsilon f_1(w)$$\n",
        "\n",
        "$$f_1(w) = - \\dfrac{1}{4m^2} \\nabla \\displaystyle \\sum_{j = 0}^{m-1} \\| \\nabla \\hat{C}_j(w_0) \\|^2$$\n",
        "\n",
        "$$ \\dot{w} = - \\nabla \\tilde{C}_{SGD}(w)$$\n",
        "\n",
        "$$- \\nabla \\tilde{C}_{SGD}(w) = - \\nabla C(w) - \\dfrac{\\epsilon}{4m} \\nabla \\displaystyle \\sum_{j = 0}^{m-1} \\| \\nabla \\hat{C}_j(w_0) \\|^2$$\n",
        "\n",
        "\n",
        "$$\\tilde{C}_{SGD}(w) = C(w) + \\dfrac{\\epsilon}{4m} \\displaystyle \\sum_{j = 0}^{m-1} \\| \\nabla \\hat{C}_j(w_0) \\|^2 \\qquad  \\blacksquare$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFQuEY-7QGxh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ddhUQ_hsVDg"
      },
      "source": [
        "## Begin"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22PWARf2sZDi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dBM0TO7RNwE3"
      },
      "source": [
        "### Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI9cpExI8U9l",
        "outputId": "912770f6-56e2-4d7b-9e67-b8dd9fd5229a"
      },
      "source": [
        "!pip install mxnet\n",
        "\n",
        "%matplotlib inline\n",
        "from __future__ import print_function\n",
        "import mxnet as mx\n",
        "import mxnet.ndarray as nd\n",
        "from mxnet import nd, autograd, gluon\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "ctx = mx.cpu()\n",
        "mx.random.seed(1)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting mxnet\n",
            "  Downloading mxnet-1.8.0.post0-py2.py3-none-manylinux2014_x86_64.whl (46.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 46.9 MB 39 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (2.23.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>1.16.0 in /usr/local/lib/python3.7/dist-packages (from mxnet) (1.19.5)\n",
            "Collecting graphviz<0.9.0,>=0.8.1\n",
            "  Downloading graphviz-0.8.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.20.0->mxnet) (2021.5.30)\n",
            "Installing collected packages: graphviz, mxnet\n",
            "  Attempting uninstall: graphviz\n",
            "    Found existing installation: graphviz 0.10.1\n",
            "    Uninstalling graphviz-0.10.1:\n",
            "      Successfully uninstalled graphviz-0.10.1\n",
            "Successfully installed graphviz-0.8.4 mxnet-1.8.0.post0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqfcrBf3SJbS"
      },
      "source": [
        ""
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-q8XaPdpOGkS"
      },
      "source": [
        "### Download data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xo1w0fS5-_j5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "901b330b-bcc3-4cee-c1b3-b14db5269c79"
      },
      "source": [
        "batch_sizes = 32 # 16 # 64\n",
        "\n",
        "def transform(data, label):\n",
        "    return nd.transpose(data.astype(np.float32), (2,0,1))/255, label.astype(np.float32)\n",
        "train_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=True, transform=transform),\n",
        "                                      batch_sizes, shuffle=True)\n",
        "test_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=False, transform=transform),\n",
        "                                     batch_sizes, shuffle=False)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading /root/.mxnet/datasets/mnist/train-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-images-idx3-ubyte.gz...\n",
            "Downloading /root/.mxnet/datasets/mnist/train-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/train-labels-idx1-ubyte.gz...\n",
            "Downloading /root/.mxnet/datasets/mnist/t10k-images-idx3-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-images-idx3-ubyte.gz...\n",
            "Downloading /root/.mxnet/datasets/mnist/t10k-labels-idx1-ubyte.gz from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/mnist/t10k-labels-idx1-ubyte.gz...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFs-r1M5SIz2"
      },
      "source": [
        ""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QI7UwfNNjL5"
      },
      "source": [
        "### Setting parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mza1ABM4_VSi"
      },
      "source": [
        "num_inputs = 784\n",
        "num_outputs = 10\n",
        "weight_scale = .01\n",
        "num_fc = 128\n",
        "conv_layer1 = 20\n",
        "conv_layer2 = 50\n",
        "\n",
        "W1 = nd.random_normal(shape = (conv_layer1, 1, 3,3), scale = weight_scale, ctx = ctx)\n",
        "b1 = nd.random_normal(shape = conv_layer1, scale = weight_scale, ctx = ctx)\n",
        "\n",
        "W2 = nd.random_normal(shape = (conv_layer2, conv_layer1, 5, 5), scale = weight_scale, ctx = ctx)\n",
        "b2 = nd.random_normal(shape = conv_layer2, scale = weight_scale, ctx = ctx)\n",
        "\n",
        "W3 = nd.random_normal(shape = (800, num_fc), scale = weight_scale, ctx = ctx)\n",
        "b3 = nd.random_normal(shape = num_fc, scale = weight_scale, ctx = ctx)\n",
        "\n",
        "W4 = nd.random_normal(shape = (num_fc, num_outputs), scale = weight_scale, ctx = ctx)\n",
        "b4 = nd.random_normal(shape = num_outputs, scale = weight_scale, ctx = ctx)\n",
        "\n",
        "params = [W1, b1, W2, b2, W3, b3, W4, b4]\n",
        "\n",
        "for param in params:\n",
        "  param.attach_grad()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XmbX0JqQ_wE"
      },
      "source": [
        ""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KJ87opCTVNX"
      },
      "source": [
        "### Data and label"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxXYqLg6_37q",
        "outputId": "c3af512b-ad6f-41e6-a725-141cf002722d"
      },
      "source": [
        "for data, label in train_data:\n",
        "  data = data.as_in_context(ctx)\n",
        "  print(data.shape)\n",
        "  break"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(32, 1, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhhudqWqTwsJ"
      },
      "source": [
        ""
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe7OAq5dYv8Q"
      },
      "source": [
        "### Activation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPsXqN02YufX"
      },
      "source": [
        "# Relu Activation Function\n",
        "\n",
        "def relu(X):\n",
        "  return nd.maximum(X,nd.zeros_like(X))"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3yWp1_4HY0qb"
      },
      "source": [
        ""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJCQMtBuY1cN"
      },
      "source": [
        "# Softmax Activation Function\n",
        "\n",
        "def softmax(y_linear):\n",
        "  exp = nd.exp(y_linear-nd.max(y_linear))\n",
        "  partition = nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
        "  return exp / partition"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5yurfgUhCPW"
      },
      "source": [
        ""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6PRs6gavZs3N"
      },
      "source": [
        "#### Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AHI8-8NwZSjA"
      },
      "source": [
        "# Softmax cross-entropy loss function\n",
        "\n",
        "def softmax_cross_entropy(yhat, y):\n",
        "    return - nd.nansum(y * nd.log_softmax(yhat), axis=0, exclude=True)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q5txxm9cZ9bS"
      },
      "source": [
        ""
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Synp7bFGZ9ER"
      },
      "source": [
        "# def modif_loss(yhat, y, lr, m):\n",
        "#   for param in params:\n",
        "#     C = - softmax_cross_entropy(yhat, y) + (lr / 4 * m) * nd.nansum(nd.norm(param.grad, ord = 2) ** 2, axis = 0, exclude=True)\n",
        "#   return C"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2soqVDR5W_B"
      },
      "source": [
        "### Implicit Regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Daa6MNCk5Wbo"
      },
      "source": [
        "def penalty_funct(params):\n",
        "    penalty = nd.zeros(shape=1)\n",
        "    for param in params:\n",
        "        penalty = penalty + nd.sum(nd.norm(param.grad, ord = 2) ** 2)\n",
        "    return penalty"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nIwWysF9lvWO"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tabh7RagT1qb"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frx2w5q-T41y"
      },
      "source": [
        "# model\n",
        "\n",
        "def mlp_model(X, debug=False):\n",
        "    \n",
        "  h1_conv = nd.Convolution(data=X, weight=W1, bias=b1, kernel=(3,3), num_filter=conv_layer1) #  The computation of the first convolutional layer\n",
        "\n",
        "  h1_activation = relu(h1_conv)\n",
        "\n",
        "  h1 = nd.Pooling(data=h1_activation, pool_type=\"avg\", kernel=(2,2), stride=(2,2))\n",
        "\n",
        "  if debug:\n",
        "    print(\"h1: %s\" % (np.array(h1.shape)))\n",
        "\n",
        "\n",
        "  h2_conv = nd.Convolution(data=h1, weight=W2, bias=b2, kernel=(5,5), num_filter=conv_layer2) # The computation of the second convolutional layer\n",
        "\n",
        "  h2_activation = relu(h2_conv)\n",
        "\n",
        "  h2 = nd.Pooling(data=h2_activation, pool_type=\"avg\", kernel=(2,2), stride=(2,2))\n",
        "\n",
        "  if debug:\n",
        "    print(\"h2: %s\" % (np.array(h2.shape)))\n",
        "\n",
        "\n",
        "  h2 = nd.flatten(h2) #  Flattening h2 so that we can feed it into a fully-connected layer\n",
        "\n",
        "  if debug:\n",
        "    print(\"Flatten h2: %s\" % (np.array(h2.shape)))\n",
        "\n",
        "\n",
        "  h3_linear = nd.dot(h2, W3) + b3 #  Define the computation of the third (fully-connected) layer\n",
        "\n",
        "  h3 = relu(h3_linear)\n",
        "\n",
        "  if debug:\n",
        "    print(\"h3: %s\" % (np.array(h3.shape)))\n",
        "\n",
        "  \n",
        "  yhat_linear = nd.dot(h3, W4) + b4 # The computation of the output layer\n",
        "\n",
        "  if debug:\n",
        "      print(\"y_out: %s\" % (np.array(yhat_linear.shape)))\n",
        "\n",
        "  return yhat_linear"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtMmwId5T5ch"
      },
      "source": [
        ""
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BdGfKxWCaUhQ",
        "outputId": "24ad148e-2933-4e42-8033-37ed0fd7b029"
      },
      "source": [
        "# Output Shape\n",
        "\n",
        "output = mlp_model(data, debug=True)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "h1: [32 20 13 13]\n",
            "h2: [32 50  4  4]\n",
            "Flatten h2: [ 32 800]\n",
            "h3: [ 32 128]\n",
            "y_out: [32 10]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V2nK6NbakSXd"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyFIuFv2bZds"
      },
      "source": [
        "### Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FdXWEkibcxU"
      },
      "source": [
        "def SGD(params, lr):\n",
        "    for param in params:\n",
        "        param[:] = param - lr * param.grad"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbVvhBI0bq2Q"
      },
      "source": [
        ""
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5j1MqZucsJF"
      },
      "source": [
        "### Evaluation Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s52nLPB_bqur"
      },
      "source": [
        "def evaluate_accuracy(data_iterator, mlp_model):\n",
        "  numerator = 0.\n",
        "  denominator = 0.\n",
        "  loss_avg = 0.\n",
        "  for i, (data, label) in enumerate(data_iterator):\n",
        "    data = data.as_in_context(ctx)\n",
        "    label = label.as_in_context(ctx)\n",
        "    label_one_hot = nd.one_hot(label, 10)\n",
        "    output = mlp_model(data)\n",
        "    predictions = nd.argmax(output, axis=1)\n",
        "    numerator += nd.sum(predictions == label)\n",
        "    denominator += data.shape[0]\n",
        "  return (numerator / denominator).asscalar()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bc8wc28Lbqm3"
      },
      "source": [
        ""
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHMUq30rdlys"
      },
      "source": [
        "### Training Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_D7jWEOrbqfF",
        "outputId": "38c52b35-b25a-47ae-8d3b-6d94cfd74c94"
      },
      "source": [
        "#m = round(len(train_data) / batch_sizes)\n",
        "m = len(train_data) / batch_sizes\n",
        "epochs = 5\n",
        "learning_rate = 1e-3    # 1e-2 # 2e-2\n",
        "smoothing_constant = .1\n",
        "\n",
        "for e in range(epochs):\n",
        "  for i, (data, label) in enumerate(train_data):\n",
        "    data = data.as_in_context(ctx)\n",
        "    label = label.as_in_context(ctx)\n",
        "    label_one_hot = nd.one_hot(label, num_outputs)\n",
        "    with autograd.record():\n",
        "      output = mlp_model(data)\n",
        "      loss = softmax_cross_entropy(output, label_one_hot) + (learning_rate / 4 * m) * penalty_funct(params)\n",
        "      #loss = modif_loss(output, label_one_hot, learning_rate, m)\n",
        "    loss.backward()\n",
        "    SGD(params, learning_rate)\n",
        "\n",
        "    ##########################\n",
        "    #  Keep a moving average of the losses\n",
        "    ##########################\n",
        "    curr_loss = nd.mean(loss).asscalar()\n",
        "    moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
        "                    else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
        "\n",
        "\n",
        "  test_accuracy = evaluate_accuracy(test_data, mlp_model)\n",
        "  train_accuracy = evaluate_accuracy(train_data, mlp_model)\n",
        "  print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0. Loss: 2.7418936878560882, Train_acc 0.11236667, Test_acc 0.1135\n",
            "Epoch 1. Loss: 2.724549264564333, Train_acc 0.11236667, Test_acc 0.1135\n",
            "Epoch 2. Loss: 2.7990536574785962, Train_acc 0.11236667, Test_acc 0.1135\n",
            "Epoch 3. Loss: 68.88300029657286, Train_acc 0.91723335, Test_acc 0.9177\n",
            "Epoch 4. Loss: 45.134580734495025, Train_acc 0.964, Test_acc 0.9655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkfHstkjbqW4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHGUv211sNdF"
      },
      "source": [
        "## End"
      ]
    }
  ]
}