{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ENvaSjI1rWMO"
      },
      "source": [
        "**1. How deep learning works at all**\n",
        "\n",
        "To figure out how deep learning works, it's not sufficient to focus on the loss function or the model class only. During training step, many algorithms are used to find minima namely: Gradient Descente (GD), Mini-Batch Gradient Descente (MBGD) or Stochastic Gradient Descente (SGD). But SGD seems to play an important role.\n",
        "\n",
        "For the specific training data, there are several minima, some of them generalise well (ie result in low test error) others can be arbitrarly badly overfit.\n",
        "\n",
        "In this kind of situation, one is interested in whether the optimisation algorithm converges quickly to a local minimum (such as a general principle) but here the interest is in which of the available minima it prefers to reach first. \n",
        "\n",
        "Optimisation algorithms have certain preference in their convergence to a minimum among the possible available minima and this preference is often described as an \"implicit regularization\"."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-A2S0tNnxml7"
      },
      "source": [
        "**2. State of the art**\n",
        "\n",
        "When we train the deep learning model, the learning rate plays in some case an important role. Managing the learning rate can significantly achieve the performance both in test and train accuracies. Large learning rate can give the high test accuracy and this effect can minimize the training loss. It's often difficult to generalize this phenomenom, since the learning rate which maximizes test accuracy is often larger than the learning rate which minimizes training loss.\n",
        "\n",
        "To interpret this phenomenom, we use the SGD by modifying the loss function. \n",
        "\n",
        "This modification consists to combine the original loss function with an implicit regularizer which penalizes the norms of the Mini-Batch Gradients. All this modification is done under some assumptions, when the batch size is small the scale of the implicit regularization term is proportional to the ratio of the learning rate to the batch size.\n",
        "\n",
        "The modified loss is:\n",
        "\n",
        "$$C_{modSGD}(w) = C_{Org}(w) + \\dfrac{\\epsilon}{4m}\\displaystyle\\sum_{j = 1}^{m}\\|{\\nabla C_j(w)}\\|^2$$\n",
        "\n",
        "Where $m = \\dfrac{N}{B}$ defines the number of batches per epoch, B: batch size and N: The training set size."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "kxfQzhWxAM3P"
      },
      "source": [
        "**3. Core idea**\n",
        "\n",
        "Modified the loss function in order to see how the small and finite learning rate can aid generalization.\n",
        "\n",
        "This novel approach is called **implicit regularization**, this technique is established for analysing optimization algorithms (especially SGD) with finite and small step or learning rate. SGD with random shuffling, iterate also stays close to the path of gradient flow if the learning rate is small and finite, but on a modified loss."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EkKjorrwEDDO"
      },
      "source": [
        "**4. How to approach the paper in term of the implementation ?**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RugXnh1UrUeK"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aI9cpExI8U9l",
        "outputId": "364d6870-dd27-427b-ee84-811297563a52"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import mxnet as mx\n",
        "import mxnet.ndarray as nd\n",
        "from mxnet import nd, autograd, gluon\n",
        "import numpy as np\n",
        "ctx = mx.cpu()\n",
        "mx.random.seed(1)\n",
        "\n",
        "\n",
        "# for plotting purposes\n",
        "%matplotlib inline\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xo1w0fS5-_j5"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "num_inputs = 784\n",
        "num_outputs = 10\n",
        "def transform(data, label):\n",
        "    return nd.transpose(data.astype(np.float32), (2,0,1))/255, label.astype(np.float32)\n",
        "train_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=True, transform=transform),\n",
        "                                      batch_size, shuffle=True)\n",
        "test_data = gluon.data.DataLoader(gluon.data.vision.MNIST(train=False, transform=transform),\n",
        "                                     batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mza1ABM4_VSi"
      },
      "outputs": [
        {
          "ename": "MXNetError",
          "evalue": "Traceback (most recent call last):\n  File \"C:\\Jenkins\\workspace\\mxnet-tag\\mxnet\\src\\resource.cc\", line 167\nMXNetError: GPU is not enabled",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mMXNetError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5824\\3381258608.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mnum_filter_conv_layer2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m50\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mW1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_filter_conv_layer1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mb1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom_normal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnum_filter_conv_layer1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mweight_scale\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\22827\\.conda\\envs\\tmp\\lib\\site-packages\\mxnet\\ndarray\\register.py\u001b[0m in \u001b[0;36mrandom_normal\u001b[1;34m(loc, scale, shape, ctx, dtype, out, name, **kwargs)\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\22827\\.conda\\envs\\tmp\\lib\\site-packages\\mxnet\\_ctypes\\ndarray.py\u001b[0m in \u001b[0;36m_imperative_invoke\u001b[1;34m(handle, ndargs, keys, vals, out, is_np_op, output_is_list)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[0mc_str_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvals\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 91\u001b[1;33m         ctypes.byref(out_stypes)))\n\u001b[0m\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m     \u001b[0mcreate_ndarray_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_global_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_np_ndarray_cls\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mis_np_op\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0m_global_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ndarray_cls\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\22827\\.conda\\envs\\tmp\\lib\\site-packages\\mxnet\\base.py\u001b[0m in \u001b[0;36mcheck_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    244\u001b[0m     \"\"\"\n\u001b[0;32m    245\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 246\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mget_last_ffi_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    247\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mMXNetError\u001b[0m: Traceback (most recent call last):\n  File \"C:\\Jenkins\\workspace\\mxnet-tag\\mxnet\\src\\resource.cc\", line 167\nMXNetError: GPU is not enabled"
          ]
        }
      ],
      "source": [
        "# Parameters\n",
        "\n",
        "#######################\n",
        "#  Set the scale for weight initialization and choose\n",
        "#  the number of hidden units in the fully-connected layer\n",
        "#######################\n",
        "\n",
        "weight_scale = .01\n",
        "num_fc = 128\n",
        "num_filter_conv_layer1 = 20\n",
        "num_filter_conv_layer2 = 50\n",
        "\n",
        "W1 = nd.random_normal(shape=(num_filter_conv_layer1, 1, 3,3), scale=weight_scale, ctx=ctx)\n",
        "b1 = nd.random_normal(shape=num_filter_conv_layer1, scale=weight_scale, ctx=ctx)\n",
        "\n",
        "W2 = nd.random_normal(shape=(num_filter_conv_layer2, num_filter_conv_layer1, 5, 5),\n",
        "                                                    scale=weight_scale, ctx=ctx)\n",
        "b2 = nd.random_normal(shape=num_filter_conv_layer2, scale=weight_scale, ctx=ctx)\n",
        "\n",
        "W3 = nd.random_normal(shape=(800, num_fc), scale=weight_scale, ctx=ctx)\n",
        "b3 = nd.random_normal(shape=num_fc, scale=weight_scale, ctx=ctx)\n",
        "\n",
        "W4 = nd.random_normal(shape=(num_fc, num_outputs), scale=weight_scale, ctx=ctx)\n",
        "b4 = nd.random_normal(shape=num_outputs, scale=weight_scale, ctx=ctx)\n",
        "\n",
        "params = [W1, b1, W2, b2, W3, b3, W4, b4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LZWpWuG_VOC"
      },
      "outputs": [],
      "source": [
        "for param in params:\n",
        "    param.attach_grad()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rxXYqLg6_37q",
        "outputId": "895a4bdf-eac3-4959-fc6f-dc479a9b4b03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 20, 26, 26)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "for data, _ in train_data:\n",
        "    data = data.as_in_context(ctx)\n",
        "    break\n",
        "conv = nd.Convolution(data=data, weight=W1, bias=b1, kernel=(3,3), num_filter=num_filter_conv_layer1)\n",
        "print(conv.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZ2JH3Yl_34M",
        "outputId": "ebd563ab-38f4-4c90-88c2-a9754932ef97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 20, 13, 13)\n"
          ]
        }
      ],
      "source": [
        "# Average pooling\n",
        "\n",
        "pool = nd.Pooling(data=conv, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n",
        "print(pool.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VZ87gmvR_30b"
      },
      "outputs": [],
      "source": [
        "# Activation function\n",
        "\n",
        "def relu(X):\n",
        "    return nd.maximum(X,nd.zeros_like(X))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZNGTlpO_3wN"
      },
      "outputs": [],
      "source": [
        "# Softmax output\n",
        "\n",
        "def softmax(y_linear):\n",
        "    exp = nd.exp(y_linear-nd.max(y_linear))\n",
        "    partition = nd.sum(exp, axis=0, exclude=True).reshape((-1,1))\n",
        "    return exp / partition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H5hH91iA830q"
      },
      "outputs": [],
      "source": [
        "# Softmax cross-entropy loss\n",
        "\n",
        "def softmax_cross_entropy(yhat_linear, y):\n",
        "    return - nd.nansum(y * nd.log_softmax(yhat_linear), axis=0, exclude=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ecbo2saUv9h8"
      },
      "outputs": [],
      "source": [
        "# # Model\n",
        "\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.conv1 = nn.Conv2d(3, 6, 5)\n",
        "#         self.pool = nn.MaxPool2d(2, 2)\n",
        "#         self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "#         self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "#         self.fc2 = nn.Linear(120, 84)\n",
        "#         self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.pool(F.relu(self.conv1(x)))\n",
        "#         x = self.pool(F.relu(self.conv2(x)))\n",
        "#         x = x.view(-1, 16 * 5 * 5)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = F.relu(self.fc2(x))\n",
        "#         x = self.fc3(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "# net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whT9TLQyLx6w"
      },
      "outputs": [],
      "source": [
        "# model\n",
        "\n",
        "def net(X, debug=False):\n",
        "    ########################\n",
        "    #  Define the computation of the first convolutional layer\n",
        "    ########################\n",
        "    h1_conv = nd.Convolution(data=X, weight=W1, bias=b1, kernel=(3,3),\n",
        "                                  num_filter=num_filter_conv_layer1)\n",
        "    h1_activation = relu(h1_conv)\n",
        "    h1 = nd.Pooling(data=h1_activation, pool_type=\"avg\", kernel=(2,2), stride=(2,2))\n",
        "    if debug:\n",
        "        print(\"h1 shape: %s\" % (np.array(h1.shape)))\n",
        "\n",
        "    ########################\n",
        "    #  Define the computation of the second convolutional layer\n",
        "    ########################\n",
        "    h2_conv = nd.Convolution(data=h1, weight=W2, bias=b2, kernel=(5,5),\n",
        "                                  num_filter=num_filter_conv_layer2)\n",
        "    h2_activation = relu(h2_conv)\n",
        "    h2 = nd.Pooling(data=h2_activation, pool_type=\"avg\", kernel=(2,2), stride=(2,2))\n",
        "    if debug:\n",
        "        print(\"h2 shape: %s\" % (np.array(h2.shape)))\n",
        "\n",
        "    ########################\n",
        "    #  Flattening h2 so that we can feed it into a fully-connected layer\n",
        "    ########################\n",
        "    h2 = nd.flatten(h2)\n",
        "    if debug:\n",
        "        print(\"Flat h2 shape: %s\" % (np.array(h2.shape)))\n",
        "\n",
        "    ########################\n",
        "    #  Define the computation of the third (fully-connected) layer\n",
        "    ########################\n",
        "    h3_linear = nd.dot(h2, W3) + b3\n",
        "    h3 = relu(h3_linear)\n",
        "    if debug:\n",
        "        print(\"h3 shape: %s\" % (np.array(h3.shape)))\n",
        "\n",
        "    ########################\n",
        "    #  Define the computation of the output layer\n",
        "    ########################\n",
        "    yhat_linear = nd.dot(h3, W4) + b4\n",
        "    #yhat_linear = softmax(yhat_linear)\n",
        "    if debug:\n",
        "        print(\"yhat_linear shape: %s\" % (np.array(yhat_linear.shape)))\n",
        "\n",
        "    return yhat_linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-hegPsOM2am",
        "outputId": "3f16d200-717b-4a62-e18e-18ac04e9c96e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "h1 shape: [64 20 13 13]\n",
            "h2 shape: [64 50  4  4]\n",
            "Flat h2 shape: [ 64 800]\n",
            "h3 shape: [ 64 128]\n",
            "yhat_linear shape: [64 10]\n"
          ]
        }
      ],
      "source": [
        "# Test Run \n",
        "\n",
        "output = net(data, debug=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl0X4R_jM_k9"
      },
      "outputs": [],
      "source": [
        "# Optimizer\n",
        "\n",
        "def SGD(params, lr):\n",
        "    for param in params:\n",
        "        param[:] = param - lr * param.grad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ccD8pXnANOuP"
      },
      "outputs": [],
      "source": [
        "# The Metric Evaluation\n",
        "\n",
        "def evaluate_accuracy(data_iterator, net):\n",
        "    numerator = 0.\n",
        "    denominator = 0.\n",
        "    loss_avg = 0.\n",
        "    for i, (data, label) in enumerate(data_iterator):\n",
        "        data = data.as_in_context(ctx)\n",
        "        label = label.as_in_context(ctx)\n",
        "        label_one_hot = nd.one_hot(label, 10)\n",
        "        output = net(data)\n",
        "        predictions = nd.argmax(output, axis=1)\n",
        "        numerator += nd.sum(predictions == label)\n",
        "        denominator += data.shape[0]\n",
        "    return (numerator / denominator).asscalar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0LnBqLCOWFq"
      },
      "outputs": [],
      "source": [
        "# def evaluate_accuracy(data_iterator, net):\n",
        "#     numerator = 0.\n",
        "#     denominator = 0.\n",
        "#     loss_avg = 0.\n",
        "#     for i, (data, label) in enumerate(data_iterator):\n",
        "#         data = data.as_in_context(ctx).reshape((-1,784))\n",
        "#         label = label.as_in_context(ctx)\n",
        "#         label_one_hot = nd.one_hot(label, 10)\n",
        "#         output = net(data)\n",
        "#         loss = cross_entropy(output, label_one_hot)\n",
        "#         predictions = nd.argmax(output, axis=1)\n",
        "#         numerator += nd.sum(predictions == label)\n",
        "#         denominator += data.shape[0]\n",
        "#         loss_avg = loss_avg*i/(i+1) + nd.mean(loss).asscalar()/(i+1)\n",
        "#     return (numerator / denominator).asscalar(), loss_avg\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98u-IFKBNVyl",
        "outputId": "bf3c5e6e-714c-4620-8016-8474783cb950"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5824\\1085286827.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m#  Keep a moving average of the losses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m##########################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mcurr_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masscalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         moving_loss = (curr_loss if ((i == 0) and (e == 0))\n\u001b[0;32m     23\u001b[0m                        else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
            "\u001b[1;32mc:\\Users\\22827\\.conda\\envs\\tmp\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2583\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"The current array is not a scalar\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2584\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2585\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2586\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2587\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\22827\\.conda\\envs\\tmp\\lib\\site-packages\\mxnet\\ndarray\\ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2564\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2565\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2566\u001b[1;33m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[0;32m   2567\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2568\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = .01\n",
        "smoothing_constant = .01\n",
        "\n",
        "for e in range(epochs):\n",
        "    for i, (data, label) in enumerate(train_data):\n",
        "        data = data.as_in_context(ctx)\n",
        "        label = label.as_in_context(ctx)\n",
        "        label_one_hot = nd.one_hot(label, num_outputs)\n",
        "        with autograd.record():\n",
        "            output = net(data)\n",
        "            loss = softmax_cross_entropy(output, label_one_hot)\n",
        "        loss.backward()\n",
        "        SGD(params, learning_rate)\n",
        "\n",
        "        ##########################\n",
        "        #  Keep a moving average of the losses\n",
        "        ##########################\n",
        "        curr_loss = nd.mean(loss).asscalar()\n",
        "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
        "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
        "\n",
        "\n",
        "    test_accuracy = evaluate_accuracy(test_data, net)\n",
        "    train_accuracy = evaluate_accuracy(train_data, net)\n",
        "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiQOMUD6U1rn"
      },
      "outputs": [],
      "source": []
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-szI0O2rWUKs"
      },
      "source": [
        "*Implicit regularization*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmHFRZlEp-tQ"
      },
      "outputs": [],
      "source": [
        "# implicit regularization\n",
        "\n",
        "batch = len(train_data) / batch_size\n",
        "\n",
        "def l2_penalty(params):\n",
        "    penalty = nd.zeros(shape=1)\n",
        "    for param in params:\n",
        "        penalty = penalty + (1 / 4 * batch) * nd.sum(nd.norm(param.grad, ord = 2) ** 2)\n",
        "    return penalty"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSIFFwdPU2QD"
      },
      "outputs": [],
      "source": [
        "# for param in params:\n",
        "#     param[:] = nd.random_normal(shape=param.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XQ5T0-d_Vetu",
        "outputId": "15316000-8f3c-4247-ad3c-1f6a6f7e7485"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0. Loss: 33.35400507958277, Train_acc 0.11236667, Test_acc 0.1135\n",
            "Epoch 1. Loss: 34.8734357119291, Train_acc 0.09871667, Test_acc 0.098\n",
            "Epoch 2. Loss: 34.24208737771534, Train_acc 0.09751666, Test_acc 0.0974\n",
            "Epoch 3. Loss: 33.67863074317033, Train_acc 0.09751666, Test_acc 0.0974\n",
            "Epoch 4. Loss: 34.30400169363496, Train_acc 0.11236667, Test_acc 0.1135\n"
          ]
        }
      ],
      "source": [
        "# Training Loop\n",
        "\n",
        "epochs = 5\n",
        "learning_rate = 2e-2\n",
        "smoothing_constant = .01\n",
        "#l2_strength = learning_rate / (4 * batch_size)\n",
        "l2_strength = 2e-2\n",
        "\n",
        "for e in range(epochs):\n",
        "    for i, (data, label) in enumerate(train_data):\n",
        "        data = data.as_in_context(ctx)\n",
        "        label = label.as_in_context(ctx)\n",
        "        label_one_hot = nd.one_hot(label, num_outputs)\n",
        "        with autograd.record():\n",
        "            output = net(data)\n",
        "            loss = softmax_cross_entropy(output, label_one_hot) + l2_strength * l2_penalty(params)\n",
        "        loss.backward()\n",
        "        SGD(params, learning_rate)\n",
        "\n",
        "        ##########################\n",
        "        #  Keep a moving average of the losses\n",
        "        ##########################\n",
        "        curr_loss = nd.mean(loss).asscalar()\n",
        "        moving_loss = (curr_loss if ((i == 0) and (e == 0))\n",
        "                       else (1 - smoothing_constant) * moving_loss + (smoothing_constant) * curr_loss)\n",
        "\n",
        "\n",
        "    test_accuracy = evaluate_accuracy(test_data, net)\n",
        "    train_accuracy = evaluate_accuracy(train_data, net)\n",
        "    print(\"Epoch %s. Loss: %s, Train_acc %s, Test_acc %s\" % (e, moving_loss, train_accuracy, test_accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_omHrqxLBbnj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Implicit Regularisation_KIBINDA",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
